{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Copyright 2020 NVIDIA Corporation\n",
    "# SPDX-License-Identifier: Apache-2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import shutil\n",
    "import warnings\n",
    "import importlib\n",
    "import urllib.request as request\n",
    "from contextlib import closing\n",
    "from numba import cuda\n",
    "\n",
    "import cupy\n",
    "import cudf\n",
    "import cuml\n",
    "\n",
    "import dask_cudf\n",
    "import dask_ml\n",
    "import dask\n",
    "\n",
    "from cuml.manifold import UMAP as cuUMAP\n",
    "from cuml.dask.decomposition import PCA as cuDaskPCA\n",
    "from cuml.dask.cluster import KMeans as cuDaskKMeans\n",
    "from cuml.dask.manifold import UMAP as cuDaskUMAP\n",
    "\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from dask_cuda import initialize, LocalCUDACluster\n",
    "from dask_cuda.local_cuda_cluster import cuda_visible_devices\n",
    "\n",
    "from bokeh.io.export import export_png\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.models.tickers import FixedTicker\n",
    "from bokeh.io import output_notebook, push_notebook, show\n",
    "\n",
    "import nvidia.cheminformatics.chembldata as chembldata\n",
    "\n",
    "warnings.filterwarnings('ignore', 'Expected ')\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurations and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please add or remove device ids that can be used\n",
    "# CUDA_VISIBLE_DEVICES=[0]\n",
    "CUDA_VISIBLE_DEVICES = cuda_visible_devices(0).split(',')\n",
    "\n",
    "pca_comps = 64\n",
    "n_clusters = 6\n",
    "n_neighbors=100\n",
    "num_mols=5000\n",
    "\n",
    "enable_tcp_over_ucx = True\n",
    "enable_nvlink = False\n",
    "enable_infiniband = False\n",
    "\n",
    "COLORS = [\"#406278\", \"#e32636\", \"#9966cc\", \"#cd9575\", \"#915c83\", \"#008000\",\n",
    "          \"#ff9966\", \"#848482\", \"#8a2be2\", \"#de5d83\", \"#800020\", \"#e97451\",\n",
    "          \"#5f9ea0\", \"#36454f\", \"#008b8b\", \"#e9692c\", \"#f0b98d\", \"#ef9708\",\n",
    "          \"#0fcfc0\", \"#9cded6\", \"#d5eae7\", \"#f3e1eb\", \"#f6c4e1\", \"#f79cd4\"]\n",
    "FINGER_PRINT_FILES = 'filter_*.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download ChEMBL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/data/db\"\n",
    "db_file = os.path.join(data_dir, 'chembl_27.db')\n",
    "\n",
    "if not os.path.exists(db_file):\n",
    "    print('Downloading ChEMBL db...')\n",
    "\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    with closing(request.urlopen('ftp://ftp.ebi.ac.uk/pub/databases/chembl/ChEMBLdb/latest/chembl_27_sqlite.tar.gz')) as r:\n",
    "        with open(db_file, 'wb') as f:\n",
    "            shutil.copyfileobj(r, f)\n",
    "\n",
    "    print('Download completed')\n",
    "else:\n",
    "    print('Reusing available ChEMBL db at', db_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>ucx://127.0.0.1:54643</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:9001/status' target='_blank'>http://127.0.0.1:9001/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>1</li>\n",
       "  <li><b>Cores: </b>1</li>\n",
       "  <li><b>Memory: </b>49.27 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'ucx://127.0.0.1:54643' processes=1 threads=1, memory=49.27 GB>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster = LocalCUDACluster(protocol=\"ucx\",\n",
    "                           dashboard_address=':9001',\n",
    "                           # TODO: automate visible device list\n",
    "                           CUDA_VISIBLE_DEVICES=CUDA_VISIBLE_DEVICES,\n",
    "                           enable_tcp_over_ucx=enable_tcp_over_ucx,\n",
    "                           enable_nvlink=enable_nvlink,\n",
    "                           enable_infiniband=enable_infiniband)\n",
    "client = Client(cluster)\n",
    "n_workers = len(client.scheduler_info()['workers'].keys())\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate fingerprint from ChEMBL\n",
    "\n",
    "The 4 in ECFP4 corresponds to the diameter of the atom environments considered, while the Morgan fingerprints take a radius parameter. So a Morgan fingerprint with radius=2 is roughly equivalent to ECFP4 and FCFP4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.9 s, sys: 1.37 s, total: 14.3 s\n",
      "Wall time: 47.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import nvidia.cheminformatics.chembldata\n",
    "importlib.reload(nvidia.cheminformatics.chembldata)\n",
    "\n",
    "cache_directory = '/data/fingerprint/'\n",
    "# cache_directory = None\n",
    "from nvidia.cheminformatics.fingerprint import MorganFingerprint\n",
    "\n",
    "if cache_directory is None:\n",
    "    chem_data = chembldata.ChEmblData(db_file=db_file, fp_type=MorganFingerprint)\n",
    "    ddf = chem_data.fetch_all_props(num_recs=num_mols)\n",
    "else:\n",
    "    hdf_path = os.path.join(cache_directory, FINGER_PRINT_FILES)\n",
    "    ddf = dask.dataframe.read_hdf(hdf_path, 'fingerprints')\n",
    "\n",
    "    if num_mols > 0:\n",
    "        ddf = ddf.head(num_mols, compute=False, npartitions=-1)\n",
    "\n",
    "dcudf = dask_cudf.from_dask_dataframe(ddf)\n",
    "dcudf = dcudf.persist()\n",
    "df = dcudf.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tanimoto Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of https://en.wikipedia.org/wiki/Jaccard_index#Other_definitions_of_Tanimoto_distance\n",
    "\n",
    "@cuda.jit\n",
    "def compute_norms(data, norms):\n",
    "    i = cuda.grid(1)\n",
    "    norms[i] = len(data[i])\n",
    "    for j in range(len(data[i])):\n",
    "        if data[i][j] != 0:\n",
    "            value = j + 1\n",
    "            data[i][j] = value\n",
    "            norms[i] = norms[i] + (value**2)\n",
    "    \n",
    "    if norms[i] != 0:\n",
    "        norms[i] = math.sqrt(norms[i])\n",
    "\n",
    "@cuda.jit\n",
    "def compute_tanimoto_sim_matix(data, norms, sim_array):\n",
    "    x = cuda.grid(1)\n",
    "    rows = len(data)\n",
    "    \n",
    "    i = x // rows\n",
    "    j = x % rows\n",
    "    \n",
    "    if i == j:\n",
    "        sim_array[i][j] = 1\n",
    "        return\n",
    "    \n",
    "    a = data[i]\n",
    "    b = data[j]\n",
    "    \n",
    "    prod = 0\n",
    "    for k in range(len(data[i])):\n",
    "        prod = prod + (a[k] * b[k])\n",
    "        \n",
    "    a_norm = norms[i]\n",
    "    b_norm = norms[j]\n",
    "    \n",
    "    sim_array[i][j] = (prod / ((a_norm**2 + b_norm**2) - prod))\n",
    "    \n",
    "def tanimotoSimilarity(fp):\n",
    "    norms = cupy.zeros(fp.shape[0])\n",
    "    compute_norms.forall(norms.shape[0], 1)(fp, norms)\n",
    "\n",
    "    sim_array = cupy.zeros((fp.shape[0], fp.shape[0]), cupy.float32)\n",
    "    compute_tanimoto_sim_matix.forall(fp.shape[0] * fp.shape[0], 1)(fp, norms, sim_array)\n",
    "    return sim_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_float = cuml.KMeans(n_clusters=n_clusters, \n",
    "                           random_state=0,\n",
    "                          init='k-means||')\n",
    "\n",
    "fp = cupy.fromDlpack(df.to_dlpack())\n",
    "sim_array = tanimotoSimilarity(fp)\n",
    "df2 = cudf.DataFrame(sim_array)\n",
    "kmeans_float.fit(df2)\n",
    "clusters = kmeans_float.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Silhouette Score in Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import cudf\n",
    "import pandas\n",
    "import numpy\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "def batched_silhouette_scores(embeddings, clusters, batch_size=5000, seed=0, on_gpu=True):\n",
    "    \"\"\"Calculate silhouette score in batches on the CPU\n",
    "\n",
    "    Args:\n",
    "        embeddings (cudf.DataFrame or cupy.ndarray): input features to clustering\n",
    "        clusters (cudf.DataFrame or cupy.ndarray): cluster values for each data point\n",
    "        batch_size (int, optional): Size for batching. Defaults to 5000.\n",
    "        seed (int, optional): Random seed. Defaults to 0.\n",
    "        on_gpu (bool, optional): Input data is on GPU. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        float: mean silhouette score from batches\n",
    "    \"\"\"\n",
    "\n",
    "    if on_gpu:\n",
    "        arraylib = cupy\n",
    "        dflib = cudf\n",
    "        AsArray = cupy.asnumpy\n",
    "    else:\n",
    "        arraylib = numpy\n",
    "        dflib = pandas\n",
    "        AsArray = numpy.asarray\n",
    "\n",
    "    # Function to calculate results\n",
    "    def _silhouette_scores(input_data):\n",
    "        embeddings, clusters = input_data\n",
    "        return silhouette_score(AsArray(embeddings), AsArray(clusters))\n",
    "\n",
    "    # Shuffle on GPU\n",
    "    combined = dflib.DataFrame(embeddings) if not isinstance(embeddings, dflib.DataFrame) else embeddings\n",
    "    embeddings_columns = combined.columns\n",
    "    cluster_column = 'clusters'\n",
    "\n",
    "    clusters = dflib.Series(clusters, name=cluster_column)\n",
    "    combined[cluster_column] = clusters\n",
    "    combined = combined.sample(n=len(combined), replace=False, random_state=seed) # shuffle via sampling\n",
    "\n",
    "    embeddings = combined[embeddings_columns]\n",
    "    clusters = combined[cluster_column]\n",
    "\n",
    "    # Chunk arrays\n",
    "    if on_gpu:\n",
    "        embeddings = cupy.fromDlpack(embeddings.to_dlpack())\n",
    "        clusters = cupy.fromDlpack(clusters.to_dlpack())\n",
    "\n",
    "    n_chunks = int(math.ceil(len(embeddings) / batch_size))\n",
    "    embeddings_chunked = arraylib.array_split(embeddings, n_chunks)\n",
    "    clusters_chunked = arraylib.array_split(clusters, n_chunks)\n",
    "    \n",
    "    # Calculate scores on batches and return the average\n",
    "    scores = list(map(_silhouette_scores, zip(embeddings_chunked, clusters_chunked)))\n",
    "    return numpy.array(scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07745108"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GPU version\n",
    "df2 = cudf.DataFrame(sim_array)\n",
    "clusters = kmeans_float.labels_\n",
    "on_gpu = True\n",
    "\n",
    "batched_silhouette_scores(df2, clusters, batch_size=300, on_gpu=on_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07859744"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CPU version\n",
    "on_gpu = False\n",
    "batched_silhouette_scores(df2.to_pandas(), clusters.to_pandas(), batch_size=300, on_gpu=on_gpu)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

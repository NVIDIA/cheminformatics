{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "funded-vegetation",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'megatron_bart'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-dbe328b4fe38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmegatron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minitialize_megatron\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmegatron_molbart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msetup_model_and_optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmegatron\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nbs/megatron_molbart/train.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mapex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFusedAdam\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdamW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmegatron_bart\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMegatronBART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmolbart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDecodeSampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeepspeed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'megatron_bart'"
     ]
    }
   ],
   "source": [
    "!export PYTHONPATH=/opt/MolBART:/worspace/nbs/:.\n",
    "\n",
    "from megatron.initialize import initialize_megatron\n",
    "from megatron_molbart.train import setup_model_and_optimizer\n",
    "from megatron import get_args\n",
    "\n",
    "initialize_megatron()\n",
    "args = get_args()\n",
    "(model, optimizer, lr_scheduler) = setup_model_and_optimizer(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "informative-rally",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md\tmegatron_conda_env.yml\t   requirements.txt  train_megatron.sh\n",
      "bart_vocab.txt\tmegatron_molbart\t   setup.py\n",
      "evaluate.sh\tmegatron_requirements.txt  test\n",
      "fine_tune.sh\tmolbart\t\t\t   train.sh\n"
     ]
    }
   ],
   "source": [
    "!ls /opt/MolBART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "proud-element",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--num-layers NUM_LAYERS]\n",
      "                             [--num-unique-layers NUM_UNIQUE_LAYERS]\n",
      "                             [--param-sharing-style {grouped,spaced}]\n",
      "                             [--hidden-size HIDDEN_SIZE]\n",
      "                             [--num-attention-heads NUM_ATTENTION_HEADS]\n",
      "                             [--max-position-embeddings MAX_POSITION_EMBEDDINGS]\n",
      "                             [--make-vocab-size-divisible-by MAKE_VOCAB_SIZE_DIVISIBLE_BY]\n",
      "                             [--layernorm-epsilon LAYERNORM_EPSILON]\n",
      "                             [--apply-residual-connection-post-layernorm]\n",
      "                             [--openai-gelu] [--onnx-safe ONNX_SAFE]\n",
      "                             [--attention-dropout ATTENTION_DROPOUT]\n",
      "                             [--hidden-dropout HIDDEN_DROPOUT]\n",
      "                             [--weight-decay WEIGHT_DECAY]\n",
      "                             [--clip-grad CLIP_GRAD] [--batch-size BATCH_SIZE]\n",
      "                             [--checkpoint-activations]\n",
      "                             [--distribute-checkpointed-activations]\n",
      "                             [--checkpoint-num-layers CHECKPOINT_NUM_LAYERS]\n",
      "                             [--train-iters TRAIN_ITERS]\n",
      "                             [--log-interval LOG_INTERVAL]\n",
      "                             [--exit-interval EXIT_INTERVAL]\n",
      "                             [--tensorboard-dir TENSORBOARD_DIR]\n",
      "                             [--scaled-upper-triang-masked-softmax-fusion]\n",
      "                             [--scaled-masked-softmax-fusion]\n",
      "                             [--bias-gelu-fusion] [--bias-dropout-fusion]\n",
      "                             [--seed SEED] [--init-method-std INIT_METHOD_STD]\n",
      "                             [--lr LR]\n",
      "                             [--lr-decay-style {constant,linear,cosine,exponential}]\n",
      "                             [--lr-decay-iters LR_DECAY_ITERS]\n",
      "                             [--min-lr MIN_LR] [--warmup WARMUP]\n",
      "                             [--override-lr-scheduler]\n",
      "                             [--use-checkpoint-lr-scheduler] [--save SAVE]\n",
      "                             [--save-interval SAVE_INTERVAL] [--no-save-optim]\n",
      "                             [--no-save-rng] [--load LOAD] [--no-load-optim]\n",
      "                             [--no-load-rng] [--finetune] [--fp16]\n",
      "                             [--apply-query-key-layer-scaling]\n",
      "                             [--attention-softmax-in-fp32] [--fp32-allreduce]\n",
      "                             [--hysteresis HYSTERESIS]\n",
      "                             [--loss-scale LOSS_SCALE]\n",
      "                             [--loss-scale-window LOSS_SCALE_WINDOW]\n",
      "                             [--min-scale MIN_SCALE] [--fp16-lm-cross-entropy]\n",
      "                             [--model-parallel-size MODEL_PARALLEL_SIZE]\n",
      "                             [--distributed-backend {nccl,gloo}]\n",
      "                             [--DDP-impl {local,torch}]\n",
      "                             [--local_rank LOCAL_RANK]\n",
      "                             [--lazy-mpu-init LAZY_MPU_INIT]\n",
      "                             [--use-cpu-initialization]\n",
      "                             [--eval-iters EVAL_ITERS]\n",
      "                             [--eval-interval EVAL_INTERVAL]\n",
      "                             [--data-path DATA_PATH] [--split SPLIT]\n",
      "                             [--vocab-file VOCAB_FILE]\n",
      "                             [--merge-file MERGE_FILE]\n",
      "                             [--seq-length SEQ_LENGTH] [--mask-prob MASK_PROB]\n",
      "                             [--short-seq-prob SHORT_SEQ_PROB] [--mmap-warmup]\n",
      "                             [--num-workers NUM_WORKERS]\n",
      "                             [--tokenizer-type {BertWordPieceLowerCase,BertWordPieceCase,GPT2BPETokenizer}]\n",
      "                             [--data-impl {lazy,cached,mmap,infer}]\n",
      "                             [--reset-position-ids] [--reset-attention-mask]\n",
      "                             [--eod-mask-loss] [--adlr-autoresume]\n",
      "                             [--adlr-autoresume-interval ADLR_AUTORESUME_INTERVAL]\n",
      "                             [--ict-head-size ICT_HEAD_SIZE]\n",
      "                             [--ict-load ICT_LOAD] [--bert-load BERT_LOAD]\n",
      "                             [--titles-data-path TITLES_DATA_PATH]\n",
      "                             [--query-in-block-prob QUERY_IN_BLOCK_PROB]\n",
      "                             [--use-one-sent-docs]\n",
      "                             [--report-topk-accuracies REPORT_TOPK_ACCURACIES [REPORT_TOPK_ACCURACIES ...]]\n",
      "                             [--faiss-use-gpu]\n",
      "                             [--block-data-path BLOCK_DATA_PATH]\n",
      "                             [--indexer-batch-size INDEXER_BATCH_SIZE]\n",
      "                             [--indexer-log-interval INDEXER_LOG_INTERVAL]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /workspace/.local/share/jupyter/runtime/kernel-eab82e76-9084-4e83-8241-2f5cc1dfc9ab.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/az/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3445: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "initialize_megatron()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stylish-husband",
   "metadata": {},
   "outputs": [],
   "source": [
    "from megatron_molbart.train import \n",
    "from molbart.util import DEFAULT_CHEM_TOKEN_START\n",
    "from molbart.util import REGEX\n",
    "from molbart.util import DEFAULT_VOCAB_PATH\n",
    "\n",
    "tokenizer = MolEncTokeniser.from_vocab_file(DEFAULT_VOCAB_PATH, REGEX,\n",
    "        DEFAULT_CHEM_TOKEN_START)\n",
    "\n",
    "VOCAB_SIZE = len(tokenizer)\n",
    "MAX_SEQ_LEN = 512\n",
    "pad_token_idx = tokenizer.vocab[tokenizer.pad_token]\n",
    "sampler = DecodeSampler(tokenizer, MAX_SEQ_LEN)\n",
    "\n",
    "model = MegatronBART(\n",
    "    sampler,\n",
    "    pad_token_idx,\n",
    "    VOCAB_SIZE,\n",
    "    args.hidden_size,\n",
    "    args.num_layers,\n",
    "    args.num_attention_heads,\n",
    "    args.hidden_size * 4,\n",
    "    MAX_SEQ_LEN,\n",
    "    dropout=0.1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upset-guinea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retired-society",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimum-signature",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "automated-gambling",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "\n",
    "from molbart.models import BARTModel\n",
    "from molbart.decode import DecodeSampler\n",
    "from rdkit import Chem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indoor-kitchen",
   "metadata": {},
   "source": [
    "## Load Tokenizer and Model\n",
    "\n",
    "The following code will load a pickled tokenizer and model checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "outdoor-provider",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizer(tokenizer_path):\n",
    "    \"\"\"Load pickled tokenizer\n",
    "       \n",
    "       Params:\n",
    "           tokenizer_path: str, path to pickled tokenizer\n",
    "    \n",
    "       Returns:\n",
    "           MolEncTokeniser tokenizer object\n",
    "    \"\"\"\n",
    "    \n",
    "    tokenizer_path = Path(tokenizer_path)\n",
    "    \n",
    "    with open(tokenizer_path, 'rb') as fh:\n",
    "        tokenizer = pickle.load(fh)\n",
    "        \n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def load_model(model_checkpoint_path, tokenizer, max_seq_len):\n",
    "    \"\"\"Load saved model checkpoint\n",
    "    \n",
    "       Params:\n",
    "           model_checkpoint_path: str, path to saved model checkpoint\n",
    "           tokenizer: MolEncTokeniser tokenizer object\n",
    "           max_seq_len: int, maximum sequence length\n",
    "        \n",
    "       Returns:\n",
    "           MolBART trained model\n",
    "    \"\"\"\n",
    "    \n",
    "    sampler = DecodeSampler(tokenizer, max_seq_len)\n",
    "    pad_token_idx = tokenizer.vocab[tokenizer.pad_token]\n",
    "\n",
    "    bart_model = BARTModel.load_from_checkpoint(model_path, \n",
    "                                                decode_sampler=sampler, \n",
    "                                                pad_token_idx=pad_token_idx)\n",
    "    bart_model.sampler.device = \"cuda\"\n",
    "    return bart_model.cuda()\n",
    "    \n",
    "\n",
    "tokenizer_path = '/data/training_data/mol_opt_tokeniser.pickle'\n",
    "model_path = '/data/training_data/az_molbart_pretrain.ckpt'\n",
    "\n",
    "max_seq_len = 64\n",
    "tokenizer = load_tokenizer(tokenizer_path)\n",
    "bart_model = load_model(model_path, tokenizer, max_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessible-conviction",
   "metadata": {},
   "source": [
    "## Interpolation Functions\n",
    "\n",
    "The following are updated versions of the interpolation functions below. These versions should be used instead of those below. Key changes vs the original functions are the ability to set padding for smiles tokens and batch-wise calculation of the interpolated embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "executive-service",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smiles2embedding(smiles, tokenizer, pad_length=None):\n",
    "    \"\"\"Calculate embedding and padding mask for smiles with optional extra padding\n",
    "    \n",
    "       Params\n",
    "           smiles: string, input SMILES molecule\n",
    "           tokenizer: MolEncTokeniser tokenizer object\n",
    "           pad_length: optional extra\n",
    "           \n",
    "       Returns\n",
    "           embedding array and boolean mask\n",
    "    \"\"\"\n",
    "    \n",
    "    assert isinstance(smiles, str)\n",
    "    if pad_length:\n",
    "        assert pad_length >= len(smiles) + 2\n",
    "        \n",
    "    tokens = tokenizer.tokenise([smiles], pad=True)\n",
    "\n",
    "    # Append to tokens and mask if appropriate\n",
    "    if pad_length:\n",
    "        for i in range(len(tokens['original_tokens'])):        \n",
    "            n_pad = pad_length - len(tokens['original_tokens'][i])\n",
    "            tokens['original_tokens'][i] += [tokenizer.pad_token] * n_pad\n",
    "            tokens['pad_masks'][i] += [1] * n_pad\n",
    "\n",
    "    token_ids = torch.tensor(tokenizer.convert_tokens_to_ids(tokens['original_tokens'])).cuda().T\n",
    "    pad_mask = torch.tensor(tokens['pad_masks']).bool().cuda().T\n",
    "    encode_input = {\"encoder_input\": token_ids, \"encoder_pad_mask\": pad_mask}\n",
    "\n",
    "    embedding = bart_model.encode(encode_input)\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return embedding, pad_mask\n",
    "\n",
    "\n",
    "def interpolate_molecules(smiles1, smiles2, num_interp, tokenizer, bart_model, k=1):\n",
    "    \"\"\"Interpolate between two molecules in embedding space.\n",
    "    \n",
    "       Params\n",
    "           smiles1: str, input SMILES molecule\n",
    "           smiles2: str, input SMILES molecule\n",
    "           num_interp: int, number of molecules to interpolate\n",
    "           tokenizer: MolEncTokeniser tokenizer object\n",
    "           bart_model: MolBART trained model\n",
    "           k: number of molecules for beam search, default 1. Can increase if there are issues with validity\n",
    "           \n",
    "       Returns\n",
    "           list of interpolated smiles molecules\n",
    "    \"\"\"\n",
    "    \n",
    "    pad_length = max(len(smiles1), len(smiles2)) + 2 # add 2 for start / stop\n",
    "    embedding1, pad_mask1 = smiles2embedding(smiles1, tokenizer, pad_length=pad_length)\n",
    "    embedding2, pad_mask2 = smiles2embedding(smiles2, tokenizer, pad_length=pad_length)\n",
    "\n",
    "    scale = torch.linspace(0.0, 1.0, num_interp+2)[1:-1] # skip first and last because they're the selected molecules\n",
    "    scale = scale.unsqueeze(0).unsqueeze(-1).cuda()\n",
    "    interpolated_emb = torch.lerp(embedding1, embedding2, scale).permute(1, 0, 2).cuda()\n",
    "    combined_mask = (pad_mask1 & pad_mask2).bool().cuda()\n",
    "\n",
    "    batch_size = 1 # TODO: parallelize this loop as a batch\n",
    "    smiles_interp_list = []\n",
    "    \n",
    "    for memory in interpolated_emb:\n",
    "        decode_fn = partial(bart_model._decode_fn, mem_pad_mask=combined_mask, memory=memory)\n",
    "        mol_strs, log_lhs = bart_model.sampler.beam_decode(decode_fn, batch_size=batch_size, k=k)\n",
    "        mol_strs = sum(mol_strs, []) # flatten list\n",
    "        \n",
    "        for smiles in mol_strs:\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if (mol is not None) and (smiles not in smiles_interp_list):\n",
    "                smiles_interp_list.append(smiles)\n",
    "                break\n",
    "                \n",
    "    return smiles_interp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "utility-repository",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C1(=O)C(C)=CC(=O)C(C(=O)C)=C1',\n",
       " 'C1(=O)C(C)=C(C)C(=O)C(C)=C1O',\n",
       " 'C1(=O)C(C)=C(C)C(=O)C(C)=C1',\n",
       " 'C1(=O)C(C)=C(C)C(=O)C(C)=C1C',\n",
       " 'C1(C)=CC(=O)C(C)=C(C)C1=O',\n",
       " 'C1(C)=C(C(C)C)C(=O)C(C)=CC1=O',\n",
       " 'C1(C)=CC(=O)C(CCCCC)=C(C)C1',\n",
       " 'C1(C)=C(CCCCCC)C(=O)C=C(C)C1=O']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smiles1 = \"CC(=O)OC1=CC=CC=C1C(=O)O\"\n",
    "smiles2 = \"CC(C)CC1=CC=C(C=C1)C(C)C(=O)O\"\n",
    "num_interp = 10\n",
    "k = 1\n",
    "\n",
    "interpolate_molecules(smiles1, smiles2, num_interp, tokenizer, bart_model, k=k)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "involved-jones",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'molbart.models'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-808d896bae6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/worspace/nbs/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmolbart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_train\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBARTModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmolbart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDecodeSampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrdkit\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'molbart.models'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"/workspace/nbs\")\n",
    "\n",
    "from molbart.models.pre_train import BARTModel\n",
    "from molbart.decode import DecodeSampler\n",
    "from rdkit import Chem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endless-paste",
   "metadata": {},
   "source": [
    "## Load Tokenizer and Model\n",
    "\n",
    "The following code will load a pickled tokenizer and model checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "regular-champagne",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizer(tokenizer_path):\n",
    "    \"\"\"Load pickled tokenizer\n",
    "       \n",
    "       Params:\n",
    "           tokenizer_path: str, path to pickled tokenizer\n",
    "    \n",
    "       Returns:\n",
    "           MolEncTokeniser tokenizer object\n",
    "    \"\"\"\n",
    "    \n",
    "    tokenizer_path = Path(tokenizer_path)\n",
    "    \n",
    "    with open(tokenizer_path, 'rb') as fh:\n",
    "        tokenizer = pickle.load(fh)\n",
    "        \n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def load_model(model_checkpoint_path, tokenizer, max_seq_len):\n",
    "    \"\"\"Load saved model checkpoint\n",
    "    \n",
    "       Params:\n",
    "           model_checkpoint_path: str, path to saved model checkpoint\n",
    "           tokenizer: MolEncTokeniser tokenizer object\n",
    "           max_seq_len: int, maximum sequence length\n",
    "        \n",
    "       Returns:\n",
    "           MolBART trained model\n",
    "    \"\"\"\n",
    "    \n",
    "    sampler = DecodeSampler(tokenizer, max_seq_len)\n",
    "    pad_token_idx = tokenizer.vocab[tokenizer.pad_token]\n",
    "\n",
    "    bart_model = BARTModel.load_from_checkpoint(model_path, \n",
    "                                                decode_sampler=sampler, \n",
    "                                                pad_token_idx=pad_token_idx)\n",
    "    bart_model.sampler.device = \"cuda\"\n",
    "    return bart_model.cuda()\n",
    "    \n",
    "\n",
    "tokenizer_path = '/data/training_data/mol_opt_tokeniser.pickle'\n",
    "model_path = '/data/training_data/az_molbart_pretrain.ckpt'\n",
    "\n",
    "max_seq_len = 64\n",
    "tokenizer = load_tokenizer(tokenizer_path)\n",
    "bart_model = load_model(model_path, tokenizer, max_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capital-glossary",
   "metadata": {},
   "source": [
    "## Interpolation Functions\n",
    "\n",
    "The following are updated versions of the interpolation functions below. These versions should be used instead of those below. Key changes vs the original functions are the ability to set padding for smiles tokens and batch-wise calculation of the interpolated embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "extraordinary-thompson",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smiles2embedding(smiles, tokenizer, pad_length=None):\n",
    "    \"\"\"Calculate embedding and padding mask for smiles with optional extra padding\n",
    "    \n",
    "       Params\n",
    "           smiles: string, input SMILES molecule\n",
    "           tokenizer: MolEncTokeniser tokenizer object\n",
    "           pad_length: optional extra\n",
    "           \n",
    "       Returns\n",
    "           embedding array and boolean mask\n",
    "    \"\"\"\n",
    "    \n",
    "    assert isinstance(smiles, str)\n",
    "    if pad_length:\n",
    "        assert pad_length >= len(smiles) + 2\n",
    "        \n",
    "    tokens = tokenizer.tokenise([smiles], pad=True)\n",
    "\n",
    "    # Append to tokens and mask if appropriate\n",
    "    if pad_length:\n",
    "        for i in range(len(tokens['original_tokens'])):        \n",
    "            n_pad = pad_length - len(tokens['original_tokens'][i])\n",
    "            tokens['original_tokens'][i] += [tokenizer.pad_token] * n_pad\n",
    "            tokens['pad_masks'][i] += [1] * n_pad\n",
    "\n",
    "    token_ids = torch.tensor(tokenizer.convert_tokens_to_ids(tokens['original_tokens'])).cuda().T\n",
    "    pad_mask = torch.tensor(tokens['pad_masks']).bool().cuda().T\n",
    "    encode_input = {\"encoder_input\": token_ids, \"encoder_pad_mask\": pad_mask}\n",
    "\n",
    "    embedding = bart_model.encode(encode_input)\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return embedding, pad_mask\n",
    "\n",
    "\n",
    "def interpolate_molecules(smiles1, smiles2, num_interp, tokenizer, bart_model, k=1):\n",
    "    \"\"\"Interpolate between two molecules in embedding space.\n",
    "    \n",
    "       Params\n",
    "           smiles1: str, input SMILES molecule\n",
    "           smiles2: str, input SMILES molecule\n",
    "           num_interp: int, number of molecules to interpolate\n",
    "           tokenizer: MolEncTokeniser tokenizer object\n",
    "           bart_model: MolBART trained model\n",
    "           k: number of molecules for beam search, default 1. Can increase if there are issues with validity\n",
    "           \n",
    "       Returns\n",
    "           list of interpolated smiles molecules\n",
    "    \"\"\"\n",
    "    \n",
    "    pad_length = max(len(smiles1), len(smiles2)) + 2 # add 2 for start / stop\n",
    "    embedding1, pad_mask1 = smiles2embedding(smiles1, tokenizer, pad_length=pad_length)\n",
    "    embedding2, pad_mask2 = smiles2embedding(smiles2, tokenizer, pad_length=pad_length)\n",
    "\n",
    "    scale = torch.linspace(0.0, 1.0, num_interp+2)[1:-1] # skip first and last because they're the selected molecules\n",
    "    scale = scale.unsqueeze(0).unsqueeze(-1).cuda()\n",
    "    interpolated_emb = torch.lerp(embedding1, embedding2, scale).permute(1, 0, 2).cuda()\n",
    "    combined_mask = (pad_mask1 & pad_mask2).bool().cuda()\n",
    "\n",
    "    batch_size = 1 # TODO: parallelize this loop as a batch\n",
    "    smiles_interp_list = []\n",
    "    \n",
    "    for memory in interpolated_emb:\n",
    "        decode_fn = partial(bart_model._decode_fn, mem_pad_mask=combined_mask, memory=memory)\n",
    "        mol_strs, log_lhs = bart_model.sampler.beam_decode(decode_fn, batch_size=batch_size, k=k)\n",
    "        mol_strs = sum(mol_strs, []) # flatten list\n",
    "        \n",
    "        for smiles in mol_strs:\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if (mol is not None) and (smiles not in smiles_interp_list):\n",
    "                smiles_interp_list.append(smiles)\n",
    "                break\n",
    "                \n",
    "    return smiles_interp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "protecting-things",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C1(=O)C(C)=CC(=O)C(C(=O)C)=C1',\n",
       " 'C1(=O)C(C)=C(C)C(=O)C(C)=C1O',\n",
       " 'C1(=O)C(C)=C(C)C(=O)C(C)=C1',\n",
       " 'C1(=O)C(C)=C(C)C(=O)C(C)=C1C',\n",
       " 'C1(C)=CC(=O)C(C)=C(C)C1=O',\n",
       " 'C1(C)=C(C(C)C)C(=O)C(C)=CC1=O',\n",
       " 'C1(C)=CC(=O)C(CCCCC)=C(C)C1',\n",
       " 'C1(C)=C(CCCCCC)C(=O)C=C(C)C1=O']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smiles1 = \"CC(=O)OC1=CC=CC=C1C(=O)O\"\n",
    "smiles2 = \"CC(C)CC1=CC=C(C=C1)C(C)C(=O)O\"\n",
    "num_interp = 10\n",
    "k = 1\n",
    "\n",
    "interpolate_molecules(smiles1, smiles2, num_interp, tokenizer, bart_model, k=k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amazing-communications",
   "metadata": {},
   "source": [
    "## Previous Version\n",
    "\n",
    "These are the previous versions from Rahul M. They have been preserved for testing only. There are also several bugs in the interpolation function as noted below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "republican-african",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_molecule(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is not None:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def ORIGsmiles2embedding(smiles1, smiles2):\n",
    "    \n",
    "    # Tokenize smiles and create masks\n",
    "    tokens = tokenizer.tokenise([smiles1, smiles2], pad=True)\n",
    "    token_ids = torch.tensor(tokenizer.convert_tokens_to_ids(tokens['original_tokens'])).cuda().T\n",
    "    pad_mask = torch.tensor(tokens['pad_masks']).bool().cuda().T\n",
    "    encode_input = {\"encoder_input\": token_ids, \"encoder_pad_mask\": pad_mask}\n",
    "    \n",
    "    # Calculate the embedding\n",
    "    embedding = bart_model.encode(encode_input)\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return embedding, pad_mask\n",
    "\n",
    "\n",
    "def ORIGinterpolate_molecules(molecule1, molecule2):\n",
    "    mol_emb, mol_mask = ORIGsmiles2embedding(molecule1, molecule2)\n",
    "    interp_weights = np.linspace(0.1, 0.9, num=10) # BUG -- assumes interpolation starts/ends at 0.1 and 0.9, respectively\n",
    "    for weight in interp_weights:\n",
    "        interpolated_emb = torch.lerp(mol_emb[:, 0, :], mol_emb[:, 1, :], torch.full_like(mol_emb[:, 0, :], weight))\n",
    "        combined_mask = (mol_mask[:, 0] | mol_mask[:, 1]).unsqueeze(0).T # BUG -- will default to shortest mask instead of longest\n",
    "        interpolated_emb = interpolated_emb.unsqueeze(0).permute(1, 0, 2)\n",
    "        mem_mask = mol_mask.clone().cuda()\n",
    "        bart_model.sampler.device = \"cuda\"\n",
    "        decode_fn = partial(bart_model._decode_fn, memory=interpolated_emb.cuda(), mem_pad_mask=combined_mask.bool())\n",
    "        mol_strs, log_lhs = bart_model.sampler.beam_decode(decode_fn, 1, 1)\n",
    "        for mol in mol_strs:\n",
    "            print(\"Generated molecule: \" + str(mol[0]) + \", valid: \" + str(is_valid_molecule(mol[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "double-secondary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated molecule: C1(=O)C=CC(=O)C(CCC=C)=C1O, valid: True\n",
      "Generated molecule: C1(=O)C=CC(=O)C(CCCC)=C1, valid: True\n",
      "Generated molecule: C1(=O)C=CC(=O)C(CCCCC)=C1, valid: True\n",
      "Generated molecule: C1(=O)C=CC(=O)C(C)=C1CCC, valid: True\n",
      "Generated molecule: C1(C)=C(C)C(=O)C(C)=CC1, valid: True\n",
      "Generated molecule: C1(CCCCCC)=CC(=O)C=CC1, valid: True\n",
      "Generated molecule: C(CCCCC1=CC(=O)CC1)C=C, valid: True\n",
      "Generated molecule: C1(C)=CCC(C)=CC1=CCCC, valid: True\n",
      "Generated molecule: C1CC(C(=C)C)=CCC1CC=C, valid: True\n",
      "Generated molecule: C1CCC(CCC=C(C)C)=CC1=C, valid: True\n"
     ]
    }
   ],
   "source": [
    "ORIGinterpolate_molecules(smiles1, smiles2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capable-canvas",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format=\"%(asctime)s — %(levelname)s — %(name)s — %(message)s\", datefmt=\"%m/%d/%Y %H:%M:%S\", level=logging.INFO)\n",
    "from aitextgen import aitextgen\n",
    "from aitextgen.colab import mount_gdrive, copy_file_from_gdrive\n",
    "from aitextgen.TokenDataset import TokenDataset, merge_datasets\n",
    "from aitextgen.utils import build_gpt2_config\n",
    "from aitextgen.tokenizers import train_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train a Byte-Pair Encoding tokenizer on the ZINC250K dataset. The `train_tokenizer()` function in aitextgen wraps the training method for the `tokenizer` package from HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/19/2021 00:30:20 — INFO — aitextgen.tokenizers — Saving aitextgen-vocab.json and aitextgen-merges.txt to the current directory. You will need both files to build the GPT2Tokenizer.\n"
     ]
    }
   ],
   "source": [
    "data_file = \"zinc_valid.txt\"\n",
    "train_tokenizer(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify a Model Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's specify the model config parameters and build a small GPT-2 model (~100 million parameters in size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Config {\n",
       "  \"activation_function\": \"gelu_new\",\n",
       "  \"attn_pdrop\": 0.0,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"embd_pdrop\": 0.0,\n",
       "  \"eos_token_id\": 0,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"layer_norm_epsilon\": 1e-05,\n",
       "  \"model_type\": \"gpt2\",\n",
       "  \"n_ctx\": 512,\n",
       "  \"n_embd\": 768,\n",
       "  \"n_head\": 12,\n",
       "  \"n_inner\": null,\n",
       "  \"n_layer\": 12,\n",
       "  \"n_positions\": 512,\n",
       "  \"resid_pdrop\": 0.0,\n",
       "  \"summary_activation\": null,\n",
       "  \"summary_first_dropout\": 0.0,\n",
       "  \"summary_proj_to_labels\": true,\n",
       "  \"summary_type\": \"cls_index\",\n",
       "  \"summary_use_proj\": true,\n",
       "  \"transformers_version\": \"4.2.2\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 868\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = build_gpt2_config(vocab_size=868, max_length=512, dropout=0.0, n_embd=768, n_layer=12, n_head=12)\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate a custom GPT-2 model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the GPT-2 model using the specified config and custom tokenizer we trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/19/2021 00:30:24 — INFO — aitextgen — Constructing GPT-2 model from provided config.\n",
      "02/19/2021 00:30:26 — INFO — aitextgen — Using a custom tokenizer.\n"
     ]
    }
   ],
   "source": [
    "ai = aitextgen(config=config, vocab_file=\"aitextgen-vocab.json\", merges_file=\"aitextgen-merges.txt\", to_gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try randomly generating a molecule now - we should get junk since we haven't trained the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[0mK53BrCcccnncCCCCCnOCCOCCNCCOCCOCcSCCCS&ns+])/OnSCCNOCCSnoc8�OCCOCCNCCCNOCCOCCNNCCCOCcNOCCCCOCCCNcNCCCNSSCCCSCCOCCCNcSCCScCCCCOOCnCCCCCOCNCCCCCCNCCCcCSCCCcSCCCS+SCCScCCCCCOCC\u0019snc�\u0019\u0019\u0019\t\u0019OCNCCCCCCsncCCCCCOCC45CCCcnoc�CCOCCOCcBrCCOCCCNcCCCCCOCCCCCCCNCCCCCNONCCCOCCCNcCCCCCnCOCCOCCNCCCNS(=NCCCNS\t�CCCCCOCCCCCCCOCC+](/COCCOCC�NCCCCCCNCCCCCCCSCCOCCOCCNCCOCCCSocncOCCOCCNCSCCNCBrCcsncCCOcncccn\u0019�31�cncONC�COCCOCC�53@@](/NCCCcT�CCCc\u0019�(=�SCCCS�sncconc�CSCCNCsncCCCCCN�BrCcSCCCSCCCCCOCCNCCCCCCCCCSCOSsncNNNOCnCCCCCOCCCCCCCOCCOCCOCCNNCCOCCCCCCOCCnoc\u0019�coSCCSc+ClCc&ClCCcncnc�NCCCc@](=+])/�COCCOCC\u0003CSCCSCCOCCOCCOCCCCCCCN�\u0019�SCCCS45COCCOCC�NS�NCCCcCCCSCcSCCCS��NCCCCCCCSCCSNCCCcCCOCCNC+])(31�@@](/NCCCcCOCCOCTNCCCc\u0019CSCCCc�TNCCCcCCOCCNCCOSOCCcOCCcCSCCNCCSCCOOCCOCCN45BrCcx\tSSN(-45CCCCCOCCNNCcCCSCSCCCnNCCOncncBrCcNCCOCOCCOCCOCCOCCOCOCCOC+](/\tSCCCS��OC@@](/cocOCCS31NCCCCCCNCnOCCS�NCCCNSccccCCCcOCCSNCCO&NCCCCCC\u0001CCSNCCCcSCCScCOCCOCCCCCCCNOCCOCCOCCOCCOCSCCOCCCCCN�CSCCO�NCCCcSCCScCOCCOCCCCSCCOCCOC��NCCCNS�SCCCN@](=+])/ONCOCCOCCOCCOCCOOCCOCC)CCSCCCCCN�31\n"
     ]
    }
   ],
   "source": [
    "ai.generate(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the GPT-2 model on ZINC250K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6dec924c7ff4cd2b832c8300c0cbc2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/240133 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/19/2021 00:47:07 — INFO — aitextgen.TokenDataset — Encoding 240,133 sets of tokens from zinc_valid.txt.\n",
      "GPU available: True, used: True\n",
      "02/19/2021 00:47:12 — INFO — lightning — GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "02/19/2021 00:47:12 — INFO — lightning — TPU available: None, using: 0 TPU cores\n",
      "02/19/2021 00:47:12 — INFO — pytorch_lightning.accelerators.gpu — LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9fb41281d534889bd04f077bfedf112",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m1,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      "[)C)cc1\n",
      "<|endoftext|>O=C[C@@H]1C[C)N2ccccc2)n1\n",
      "<|endoftext|>CN[C@H](c1ccccc1\n",
      "<|endoftext|>O)c1ccccc1ccc(NC[C)C@@H]2\n",
      "<|endoftext|>O=C)[C@H](c1nnc(F)c3)[nH]c1\n",
      "<|endoftext|>O=C[C@@H](C/C@H]1(CC(C/NC(C)]2)c1ccc(F)cc1\n",
      "<|endoftext|>C(C(=O)Cc1ccc(OC)c1ccc1ccc(Cl)C)CC(=O))\n",
      "<|endoftext|>O=C[C(c1cc(ccc(-c2ccc(F)c(OC\n",
      "==========\n",
      "\u001b[1m2,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m2,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      "O)1C@H](C[C@@H]1CC(C)C)C)c2)c1\n",
      "<|endoftetext|>COc)C(C)NCc1ccccc1F\n",
      "<<|endoftext|>CCc1ccc(C)c2cc(Cl)c(C)c(Cl)cc2)nc(F)c(C)ccc2)cc1\n",
      "<|endoftext|>CC[NH+](C@H](H](C[C@H](c1cccc(N=C)C)C)c1\n",
      "<|endoftext|>CC[C@@@H]3+])3[NH2+]Cc3ccccc2+]C3)cc1\n",
      "<|endoftext|>C[C@@H](C)C[C@H](NC(=O)Cn1ccc(F)F)cc1\n",
      "<|endoftext|>O=C1C(NC(=O)Nc2\n",
      "==========\n",
      "\u001b[1m3,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m3,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      "]1C[C@@H]1CCCN1C[C@@H]2c1\n",
      "<|endoftext|>Cc1ccc(C(=O)N2CCc(C(=O)N2CCC[C@@H]2C[NH+](C2CCO2)c1\n",
      "<|endoftext|>COC=C(C(=O)C1CCC[C@@H](O)C)C)C1CC1)C1)C(=O)Nc<|endoftext|>C[C@@H](C(=O)C(=O)Oc1ccccc1Cl)C(=O)N1CCC[C@@H]1c2ccccc2C[C@@H](C)c1c1ccccc1\n",
      "<|endoftext|>C[C@@H](C)O)NC1)c1cccc(C)nc1-N-c1CCCC1\n",
      "<|endoftext|>C[C@H](C)C(=O)CN1C[\n",
      "==========\n",
      "\u001b[1m4,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m4,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      "1\n",
      "<|endoftext|>COc1cccc([C@@H]2C[NH+]Cc2cc(-c4ccccc3)n1\n",
      "<|endoftext|>COc1cc2c(c1NC(=O)N(C)C)Cc1csc(F)c1\n",
      "<|endoftext|>CN(C(=O)Cc1cccc(F)c(F)c(C1Cl\n",
      "<||endoftext|>C[C@@H](C)NC(=O)c1cccc(C(=O)NC[C@H](c2ccccc2)C2\n",
      "<|endoftext|>Cc1cccc(C)c(N2CCC(=O)C3CCCC3)c1\n",
      "<|endoftext|>COc1cc(C)cc(C(=O)[C@@H](C)NC(=O)c2nccs3ccccc3F)S2)c1\n",
      "<|endoft\n",
      "==========\n",
      "\u001b[1m5,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m5,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      "|e3CC(C)n4CCC4)CC3)CC2)cc1\n",
      "<|endoftext|>CCOC(=O)c1ccc(NC(=O)c2nc(C)c(F)c2)c(OC)c1\n",
      "<|endoftext|>COC(=O)c1ccc(F)cc1)c1ccc(F)cc1\n",
      "<|endoftext|>Cc1nc(-c2ccccc2)c(CC(=O)N2C(=O)c3ccccc3C2)c1\n",
      "<|endoftext|>C[NH+](CNC)CN(CC(=O)c1cccc(S2)c(Cl)cc1)CCCC2\n",
      "<|endoftext|>CC(C)n(C)NC(=O)c1ccc(Cl)cc1)c1cccc(Cl)c1\n",
      "<|endoftext|>C[C@H]1CCC[C@@H\n",
      "==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/19/2021 01:03:18 — INFO — aitextgen — Saving trained model pytorch_model.bin to /trained_model\n"
     ]
    }
   ],
   "source": [
    "ai.train(\"zinc_valid.txt\",\n",
    "         line_by_line=True,\n",
    "         num_steps=5000,\n",
    "         generate_every=1000,\n",
    "         save_every=1000,\n",
    "         save_gdrive=False,\n",
    "         batch_size=8,\n",
    "         n_gpu=1\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate molecules from the trained GPT-2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[0mtext|>COc1ccc(NC(=O)c2ccc(NC(=O)c3ccccc3)o2)cc1\n",
      "<|endoftext|>CC(=O)N1CCC(C(=O)NCC(Cc2ccccc2)C[C@@H](C)O1\n",
      "<|endoftext|>Cc1ccc(NC(=O)[C@@H](C)c2ccc3c(c3ccccc3)o2)cc1=O\n",
      "<|endoftext|>C[C@H]1C[C@H]([NH2+][C@@H]1C[C@H]1c1ccc(Cl)cc1\n",
      "<|endoftext|>C[C@H](NC(=O)c1nccn1Cl)c1ccccc1)c1ccccc1\n",
      "<|endoftext|>Cc1cccc(N2C(=O)c3ccc(C)c(C)c4)c(C)C)c3\n",
      "==========\n",
      "\u001b[1m\u001b[0m1\n",
      "(C)cc2cc(NC(=O)N(C)C2)C2)[C@@H](C)O[C@@H]1C(=O)[O-]\n",
      "<|endoftext|>NC(=O)c1cccc(C[NH+](C)Cc2ccccc2)c1\n",
      "<|endoftext|>C[C@@H](C(=O)NCC(=O)c1ccc(S(=O)(=O)c(Cl)c1)N1CCc2ccccc2CN1\n",
      "<|endoftext|>COc1ccc(C(=O)N2C[C@@C@@H]3c4ccccc4C2=O)cc1\n",
      "<|endoftext|>CC(C)Oc1ccc(F)cc1\n",
      "<|endoftext|>O=C(NC[C@@H]1CCOC(c2ccc(Cl)cc2)C1\n",
      "<|endoftext|>C[C\n",
      "==========\n",
      "\u001b[1m\u001b[0mc3ccc(Cl)cc3)cc2)cc1\n",
      "<|endoftext|>CC(=O)Nc1ccc(C)c(CCNC(=O)c2ccc(C)cc2)c(C)c(N)c1\n",
      "<|endoftext|>CO[C@@H]1CCC[C@@H]([NH2+][C@@H]1C[C@H](C)C(=O)OCC1\n",
      "<|endoftext|>C[C@H]1CC1)NC(=O)C[C@H]1CCC[C@@C@@H]1C\n",
      "<|endoftext|>Cc1cccc(C(=O)N(C)Cc1\n",
      "<|endoftext|>Cc1ccccc1-c1ccc(NC(=O)c2c1\n",
      "<|endoftext|>C[C@H](NC(=O)N[C@H]1CCCC[C@@H]1C\n",
      "==========\n",
      "\u001b[1m\u001b[0mtext|>C[C@H](NC(=O)c1ccc(Cl)cc1)C(=O)N[C@@H]1c2ccccc2N1c1ccc(F)cccc1\n",
      "<|endoftext|>CC(C)c1ccc(C(=O)NCc2ccc(Cl)cc2)cc1\n",
      "<|endoftext|>Cc1ccccc1NC(=O)COc1ccccc1\n",
      "<|endoftext|>CCOc1ccc(NC(=O)N2CCC(F)(F)F)cc2)cc1\n",
      "<|endoftext|>COC(=O)c1cccc(NC(=O)c2ccc(Cl)cc2)no2)c(C)c1\n",
      "<|endoftext|>CCCn(=O)(=C1=NC(=O)CO(Cc2cccs2)CC1\n",
      "<|endoftext|>O=S=C1(NC(=O)C\n",
      "==========\n",
      "\u001b[1m\u001b[0mtext|>CCc1ccc(NC(=O)Nc2ccccc2)cc(C)no1\n",
      "<|endoftext|>Cc1ccc(-n2ccnc2)cc1C[NH2+]C[C@H]1SC[C@H]1[C@@H]1C\n",
      "<|endoftext|>CC(C)C(=O)Nc1ccc(Cl)cc1\n",
      "<|endoftext|>C[C@H]1C(=O)Nc2ccccc2C1=O\n",
      "<|endoftext|>CC[C@@H](C)CC(=O)N(C)C)C)C(=O)[O-]\n",
      "<|endoftext|>Cn1ccc(NC(=O)/NCC(=O)NC(=O)C(=O)N2CCC(C)CC)CC2)c1ccccc1\n",
      "<|endoftext|>CC(=O)c1cccc(NC(=O)c\n"
     ]
    }
   ],
   "source": [
    "ai.generate(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, not all of these molecules are valid, but this is impressive given we only trained on 250K molecules for 5000 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
